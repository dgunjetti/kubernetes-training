Introduction to Kubernetes

Deepak Gunjetti
Software Architect @ Andcloud
deepak@andcloud.io
@dgunjetti

* Introduction

- Kubernetes is a container orchestrator, that automates the deployment and life-cycle management of containerized applications. 

- It orchestrates compute, storage, networking for workloads.

* Abstracts the underlying infrastructure

- It abstracts the cluster implementation from workloads

.image img/kubernetes01.png

- we can deploy on AWS, GCP, Azure or on-premise, the application remains consistent across the deployments.

* Kubernetes components

- It consists of master node and sets of worker nodes. 

- Control plane runs on master node, applications runs on worker nodes.

* Components..

.image img/components.png

* Components..

- API server is the policy engine that sits in front of etcd

- etcd is a highly consistent distributed database that holds the cluster state. 

- Controller manager is set control loops that reconcile the actual state in cluster to desired state stored in database.

- Scheduler assigns workloads to appropriate worker nodes.

- Kubelet is agent that runs on every node and makes sure that container are running and healthy.

- Container runtime is responsible to run containers, Kubernetes supports docker, rkt, containerd or any OCI compliant implementation.

* Kubernetes Primitives

* POD

* POD

- Basic unit deployment in kubernetes is a Pod.

- Pod is a wrapper over container, it represents an instance of application. 

- It encapsulates container, storage/network, and other options that govern how container is suppose to run.

- Pod can consist of single container or set of tightly coupled containers that share resources. All the containers in the Pod land on same node.

* Networking

-  Each pod has an IP address in a flat shared networking space that has full communication with other physical computers and pods across the network.

- Containers within a pod share an IP address and port space, and can talk to each other via localhost.

- The hostname is set to the pod’s Name for the application containers within the pod.

* Storage

- A Pod can specify a set of shared storage Volumes. 

- All containers in the Pod can access the shared volumes, allowing those containers to share data. 

- Volumes also allow persistent data in a Pod to survive in case one of the containers within needs to be restarted.

* Pod example

- Example of co-located containers can be, one container serving html web pages to outside world and a side-car container populating those html web pages. Both the container share a storage volume. One container writes to volume other container reads from it.

* Working with Pods

- When a Pod gets created (directly by you, or indirectly by a Controller), it is scheduled to run on a Node in your cluster. The Pod remains on that Node until the process is terminated, the pod object is deleted, the Pod is evicted for lack of resources, or the Node fails.

* Pod Templates

.code -edit src/workloads/pod01.yaml 

* Pod Lifecycle

- Pending: The Pod has been accepted by the Kubernetes system, but one or more of the Container images has not been created. This includes time before being scheduled as well as time spent downloading images over the network, which could take a while.

- Running: The Pod has been bound to a node, and all of the Containers have been created.

- Succeeded: All Containers in the Pod have terminated in success, and will not be restarted.

- Failed: All Containers in the Pod have terminated, and at least one Container has terminated in failure.  That is, the Container either exited with non-zero status.

- Unknown: For some reason the state of the Pod could not be obtained, typically due to an error in communicating with the host of the Pod.

* Restart policy

- A PodSpec has a restartPolicy field with possible values Always, OnFailure, and Never. The default value is Always. 

- restartPolicy only refers to restarts of the Containers by the kubelet on the same node.

- Exited Containers that are restarted by the kubelet are restarted with an exponential back-off delay (10s, 20s, 40s …) capped at five minutes

- once bound to a node, a Pod will not rebound to another node. A higher level contruct called controllers are used to reschedule the pods on another node in case of node failures.

* Controllers 

- Use a Job for Pods that are expected to terminate, for example, batch computations. Jobs are appropriate only for Pods with restartPolicy equal to OnFailure or Never.

- Use a ReplicationController, ReplicaSet, or Deployment for Pods that are not expected to terminate, for example, web servers. ReplicationControllers are appropriate only for Pods with a restartPolicy of Always.

- Use a DaemonSet for Pods that need to run one per machine, because they provide a machine-specific system service.

* Init Containers

- init containers are specialized containers that run before app containers in a Pod. Init containers can contain utilities or setup scripts not present in an app image. 

- Init containers always run to completion.

- Each init container must complete successfully before the next one starts.

- If a Pod’s init container fails, Kubernetes repeatedly restarts the Pod until the init container succeeds. 

* Init containers in use

    src/workloads/initcontainer01.yaml

* Pod Presets 

- PodPresets are objects for injecting certain information into pods at creation time. The information can include secrets, volumes, volume mounts, and environment variables.

- Kubernetes provides an admission controller (PodPreset) which, when enabled, applies Pod Presets to incoming pod creation requests. 

* Pod Presets...

- When a pod creation request occurs, the system does the following:

- Retrieve all PodPresets available for use.

- Check if the label selectors of any PodPreset matches the labels on the pod being created.

- Attempt to merge the various resources defined by the PodPreset into the Pod being created.

- On error, throw an event documenting the merge error on the pod, and create the pod without any injected resources from the PodPreset.

* Pod Presets...

- Create the PodPreset

    kubectl apply -f https://k8s.io/examples/podpreset/preset.yaml

    kubectl get podpreset

- Create a pod

    kubectl create -f https://k8s.io/examples/podpreset/pod.yaml

    kubectl get pods

- Pod spec after admission controller:

    kubectl get pod website -o yaml

* Disruptions



* Replicaset Controller

- Relicaset controller takes a count, and template to create Pods.

- It makes sure specified number of Pods are always running on the cluster. If any node goes down, Pods are rescheduled on different node.

- Replicaset is managed by higher level abstraction called Deployment.

* Deployment

- Deployment manages the rolling out of application. When a new version of application is to be rolled out, it creates a new Replicaset. It rolls down the Pods created by old Replicaset and rolls up the Pods on new Replicaset.

* StatefulSets

- StatefulSets attaches persistent storage with the instance of application. 

- It provides guarantee of ordering and provides unique ID to Pod.

- It is used by applications that requires stable network ID, persistent storage, graceful deployment and termination.

- Storage is either dynamically provisioned or pre-provisioned by Admin. 

- Network ID for the Pods is provided by Headless service.

* DaemonSet

- DaemonSet ensures that a Pod runs on every node in cluster. 

- Examples of these kinds of Pods are log collectors and node monitoring agents.

* Jobs

- Jobs are about tasks that need to run to completion. 

- Job need to run till it is successful.

* Cronjob

- Cronjob creates Jobs based on schedule.

* Service

- Pods are ephemeral, when the Pods are rescheduled, they get new IP address. 

- Services groups the Pods providing the same functionality and provides them a virtual IP address and ports. 

- Workloads can access these Pods using virtual IP address and port, connection is routed to one of the pod backing the service.

- Services can also provision LoadBalancers provided by cloud provider, so that functionality provided by group of Pods can be exposed to outside world.

-  LoadBalancer will redirect the traffic to virtual IP address, which redirects to one of the Pods backing the service.

- The set of pods targeted by service is determined by Label selector.

* Services Example

- There may be multiple pods that all act as the frontend, and there may be single backend database pod.

- We can create a service for frontend pods and configure that to be accessed from outside the cluster.

- Connection to service will load balance across all the backing pods.

- We can create a service for backend service. This gives stable address for the backend pod.

- This enables the frontend pod to easily find the backend service by its name.

* Creating service 

.code -edit src/services/svc01.yaml  /START OMIT/,/END OMIT/

- service my-service accepts connection on port 80 and route to port 8080 on one of pods matching app=my-app label selector.

* Creating service..

    # kubectl create -f src/services/svc01.yaml

    # kubectl  get svc
    NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
    kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP   16d
    my-service   ClusterIP   10.98.59.167   <none>        80/TCP    5m17s

- Service is assigned an Cluster IP address, which is accessible only within cluster.

- The Service’s selector will be evaluated continuously and the results will be POSTed to an Endpoints object also named “my-service”.

- Kubernetes Services support TCP, UDP and SCTP for protocols. The default is TCP.

* Access the service

- You can send requests to your service from within the cluster

    # kubectl exec my-app-7nog1 -- curl -s http://10.111.249.153

- The kubectl exec command allows you to remotely run arbitrary commands inside an existing container of a pod.

* Endpoints

- An Endpoints resource is a list of IP addresses and ports exposing a service. 

    # kubectl get ep my-service
        NAME         ENDPOINTS         AGE
        my-service   172.17.0.5:8080   95m

- The  selector is used to build a list of IPs and ports, which is then stored in the Endpoints resource.

- When a client connects to a service, the kube-proxy selects one of those IP and port pairs and redirects the incoming connection to the server listening at that location.

* Services without selectors

- Services generally abstract access to Kubernetes Pods, but they can also abstract other kinds of backends. For example:

- You want to have an external database cluster in production, but in test you use your own databases.

- You are migrating your workload to Kubernetes and some of your backends run outside of Kubernetes.

- In any of these scenarios you can define a service without a selector.

* Services without selectors...

.code -edit src/services/svc02.yaml  /START OMIT/,/END OMIT/


* Proxy mode : IPTables

- kube-proxy watches the Kubernetes master for the addition and removal of Service and Endpoints objects. 

- kube-proxy installs iptables rules which capture traffic to the Service’s clusterIP and Port 

- Redirects that traffic to one of the Service’s backend sets.

- For each Endpoints object, it installs iptables rules which select a backend Pod. By default, the choice of backend is random.

* Proxy mode : IPVS

- kube-proxy watches Kubernetes Services and Endpoints, calls netlink interface to create ipvs rules accordingly and syncs ipvs rules with Kubernetes Services and Endpoints periodically.

- When Service is accessed, traffic will be redirected to one of the backend Pods.

- IPVS is based on netfilter hook function, uses hash table as the underlying data structure and works in the kernel space.

- ipvs provides more options for load balancing algorithm, such as:
	rr: round-robin
	lc: least connection
	dh: destination hashing
	sh: source hashing
	sed: shortest expected delay
	nq: never queue

* Multi-Port Services

- When creating a service with multiple ports, you must specify a name for each port.

.code -edit src/services/svc03.yaml 

* Discovering services

- Kubernetes supports 2 primary modes of finding a Service 
    environment variables 
    DNS.

- When a Pod is run on a Node, the kubelet adds a set of environment variables for each active Service

    {SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT variables

    Ex:
    REDIS_MASTER_SERVICE_HOST=10.0.0.11
    REDIS_MASTER_SERVICE_PORT=6379

* Discovering services - dns

- The kube-system namespace includes pod core-dns.

- The Pod runs a DNS server, which all other pods running in the cluster are automatically configured to use

- The DNS server watches the Kubernetes API for new Services and creates a set of DNS records for each. 

* Headless services

- Sometimes you don’t need or want load-balancing and a single service IP. In this case, you can create “headless” services by specifying "None" for the cluster IP (.spec.clusterIP).

* Headless services - with selector

- The endpoints controller creates Endpoints records in the API, and modifies the DNS configuration to return A records (addresses) that point directly to the Pods backing the Service.

* Headless services - without selector

- The endpoints controller is not created.

- DNS system looks for and configures either:

- CNAME records for ExternalName-type services.

- A records for any Endpoints that share a name with the service.

* Service Types

- ClusterIP

- NodePort

- LoadBalancer

- ExternalName

* Service Types - ClusterIP

- Exposes the service on a cluster-internal IP

- service only reachable from within the cluster. 

- This is the default ServiceType.

* Service Types - NodePort

- Each cluster node opens a port on the node itself and redirects traffic received on that port to the underlying service. 

- Service is accessible internally via cluster-ip 

- Service is visible externally through a dedicated port on all nodes.

* Type NodePort

- type field to NodePort

- Kubernetes master will allocate a port from a range specified by --service-node-port-range flag (default: 30000-32767), and each Node will proxy that port (the same port number on every Node) into your Service. 

- That port will be reported in your Service’s .spec.ports[*].nodePort field.

- Service will be visible as both 
    
    <NodeIP>:spec.ports[*].nodePort 
    .spec.clusterIP:spec.ports[*].port

* Type NodePort

.code -edit src/services/svc04.yaml

* Type NodePort

    $ kubectl get svc my-svc-nodeport
    NAME              TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
    my-svc-nodeport   NodePort   10.102.193.100   <none>        80:30123/TCP   19s

- EXTERNAL-IP <nodes> indicates the service is accessible through the IP address of any cluster node. 

- PORT(S) column shows both the internal port of the cluster IP (80) and the node port (30123). 

- A connection received on port 30123 on a node might be forwarded either to the pod running on the same node or to one of the pods running on another node.

* Service Types - LoadBalancer

- service is accessible through a dedicated load balancer, provisioned from the cloud infrastructure 

- Clients connect to the service through the load balancer’s IP.

- The load balancer redirects traffic to the node port across all the nodes. 


* Type loadbalancer

- On cloud providers which support external load balancers, setting the type field to LoadBalancer will provision a load balancer for your Service. 

- The actual creation of the load balancer happens asynchronously, and information about the provisioned balancer will be published in the Service’s .status.loadBalancer field. 

* Type loadbalancer

.code -edit src/services/svc05.yaml

* Type loadbalancer

    $ kubectl get svc my-loadbalancer
    NAME                 CLUSTER-IP       EXTERNAL-IP      PORT(S)         AGE
    my-loadbalancer   10.111.241.153   130.211.53.173 

    $ curl http://130.211.53.173
    You've hit my-app-xueq1

- External clients connect to port 80 of the load balancer and get routed to the implicitly assigned node port on one of the nodes. From there, the connection is forwarded to one of the pod

* Service Types - ExternalName

- Maps the service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record with its value.

* External name

- If you want to access a public API, you can define a service that points to it.

.code -edit src/services/svc06.yaml 

- After the service is created, pods can connect to the external service through the external-service.default.svc.cluster.local domain name


* Liveness & Readiness Probes 

* Liveness Probes

- The kubelet uses liveness probes to know when to restart a Container. 

- For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a Container in such a state can help to make the application more available despite bugs.

* Readiness Probes

- The kubelet uses readiness probes to know when a Container is ready to start accepting traffic. 

- A Pod is considered ready when all of its Containers are ready. 

- One use of this signal is to control which Pods are used as backends for Services. 

- When a Pod is not ready, it is removed from Service load balancers.

* Liveness Probes

.code -edit src/probes/01-liveness.yaml

* Liveness Probes

- The periodSeconds field specifies that the kubelet should perform a liveness probe every 5 seconds.

- The initialDelaySeconds field tells the kubelet that it should wait 5 second before performing the first probe.

- To perform a probe, the kubelet executes the command cat /tmp/healthy in the Container.

- If the command succeeds, it returns 0, and the kubelet considers the Container to be alive and healthy. If the command returns a non-zero value, the kubelet kills the Container and restarts it.

* Liveness Probes

    kubectl create -f https://k8s.io/examples/pods/probe/exec-liveness.yaml

- After 35 seconds, view the Pod events:

    kubectl describe pod liveness-exec
    Unhealthy   Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory

    kubectl get pod liveness-exec
    NAME            READY     STATUS    RESTARTS   AGE
    liveness-exec   1/1       Running   1          1m

* Define a liveness HTTP request

.code -edit src/probes/02-http.yaml  /START OMIT/,/END OMIT/

- The kubelet sends an HTTP GET request to the server that is running in the Container and listening on port 8080. If the handler for the server’s /healthz path returns a success code, the kubelet considers the Container to be alive and healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it.

* Define a TCP liveness probe

.code -edit src/probes/03-tcp.yaml  /START OMIT/,/END OMIT/

- kubelet will attempt to open a socket to container on the specified port. If it can establish a connection, the container is considered healthy, if it can’t it is considered a failure.

* Define readiness probes

- Sometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup. In such cases, you don’t want to kill the application, but you don’t want to send it requests either.

- A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services.

- Readiness and liveness probes can be used in parallel for the same container. Using both can ensure that traffic does not reach a container that is not ready for it, and that containers are restarted when they fail.


* Kubernetes networking

- Every pod its own cluster-private-IP address

- Containers within a Pod can all reach each other’s ports on localhost

- All pods in a cluster can see each other without NAT. 

* Kubernetes networking

    $ kubectl get pods -l run=my-nginx -o wide
    NAME                        READY     STATUS    RESTARTS   IP            
    my-nginx-3800858182-jr4a2   1/1       Running   0          10.244.3.4    
    my-nginx-3800858182-kna2y   1/1       Running   0          10.244.2.5    

- You should be able to ssh into any node in your cluster and curl both IPs. 

- Note that the containers are not using port 80 on the node

- This means you can run multiple nginx pods on the same node all using the same containerPort and access them from any other pod or node in your cluster using IP.

* Securing the Service

    #create a public private key pair
    openssl req -x509 -nodes -days 365 -newkey rsa:2048 
    -keyout /tmp/nginx.key -out /tmp/nginx.crt 
    -subj "/CN=my-nginx/O=my-nginx"

    #convert the keys to base64 encoding
    cat /tmp/nginx.crt | base64
    cat /tmp/nginx.key | base64

* Securing the Service...

    src/services/secure/secret.yaml

    src/services/secure/nginx-app.yaml

- The nginx server serves HTTP traffic on port 80 and HTTPS traffic on 443, and nginx Service exposes both ports.

- Each container has access to the keys through a volume mounted at /etc/nginx/ssl. This is setup before the nginx server is started.

* Securing the Service...

    $ kubectl get pods -o yaml | grep -i podip
        podIP: 10.244.3.5
    node $ curl -k https://10.244.3.5
    ...
    <h1>Welcome to nginx!</h1>

- -k parameter is to tell curl to ignore the CName mismatch. this is because we don’t know anything about the pods running nginx at certificate generation time.

* Securing the Service...

    src/services/secure/curlpod.yaml

    $ kubectl create -f ./curlpod.yaml
    $ kubectl get pods -l app=curlpod
    NAME                               READY     STATUS    RESTARTS   AGE
    curl-deployment-1515033274-1410r   1/1       Running   0          1m
    $ kubectl exec curl-deployment-1515033274-1410r -- 
    curl https://my-nginx --cacert /etc/nginx/ssl/nginx.crt
    ...
    <title>Welcome to nginx!</title>

* Ingress

* Ingress

- Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. 

- Traffic routing is controlled by rules defined on the ingress controller.

- Ingress provides load balancing, SSL termination and name-based virtual hosting.

- Ingress controller is responsible for fulfulling the ingress, usually with load balancer.

- Ingress resource contains rules for directing HTTP traffic.

* Ingress..

.code -edit src/ingress/01-ingress.yaml  /START OMIT/,/END OMIT/

* Ingress rules

- host: if no host is specified, rules apply to all incoming request. If host is provided, the rules apply to that host.

- list of paths: each of which are associated with backend defined by serviceName and servicePort.

* Default Backend

- An Ingress with no rules sends all traffic to a single default backend. 

- The default backend is typically a configuration option of the Ingress controller

- If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend.

* Types of Ingress, Single service ingress

- exposing single service

- specifying default backend with no rules.

  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata: 
      name: test-ingress
  spec: 
      backend:
          serviceName: testsvc
          servicePort: 80

*  Single service ingress

- kubectl apply -f ingress.yaml

- kubectl get ingress test-ingress

- shows IP allocated by the Ingress controller to satisfy this Ingress.

* Simple fan out

- Routes traffic from a single IP address to more than one service, based on HTTP URI.

- Ingress allows you to keep the number of load balancer to minimum.


  http:
      paths:
      - path: /foo
      backend:
          serviceName: service1
          servicePort: 4200
      - path: /bar
      backend:
          serviceName: service2
          servicePort: 8080

- The Ingress controller provisions an load balancer that satisfies the Ingress, as long as the services (s1, s2) exist. 

* Name based virtual hosting

- Name-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address.

  foo.bar.com --|                 |-> foo.bar.com s1:80
                | 178.91.123.132  |
  bar.foo.com --|                 |-> bar.foo.com s2:80

* Name based virtual hosting..

  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: service1
          servicePort: 80
  - host: bar.foo.com
    http:
      paths:
      - backend:
          serviceName: service2
          servicePort: 80


- If you create an Ingress resource without any hosts defined in the rules, then any web traffic to the IP address of your Ingress controller can be matched without a name based virtual host being required.

* TLS

- You can secure an Ingress by specifying a Secret that contains a TLS private key and certificate. 

- Ingress controller to secure the channel from the client to the load balancer using TLS. 

  apiVersion: v1
  kind: Secret
  metadata:
    name: testsecret-tls
    namespace: default
  data:
    tls.crt: base64 encoded cert
    tls.key: base64 encoded key
  type: kubernetes.io/tls

* TLS

  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    name: tls-example-ingress
  spec:
    tls:
    - hosts:
      - sslexample.foo.com
      secretName: testsecret-tls
    rules:
      - host: sslexample.foo.com
        http:
          paths:
          - path: /
            backend:
              serviceName: service1
              servicePort: 80


- certificate is created with CN for sslexample.foo.com.


* Ingress Controllers

- In order for the Ingress resource to work, the cluster must have an ingress controller running.

- Kubernetes as a project currently supports and maintains GCE and nginx controllers.

- Additional controllers, HAProxy, Contour


* Using multiple Ingress controllers

- You may deploy any number of ingress controllers within a cluster. 

- When you create an ingress, you should annotate each ingress with the appropriate ingress.class to indicate which ingress controller should be used if more than one exists within your cluster.

* Network policy



